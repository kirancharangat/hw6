---
title: "Homework 6"
author: "Kiran Charangat"
toc: true
title-block-banner: true
title-block-style: default
execute: 
  freeze: true
  cache: true
# format:
  # html: # comment this line to get pdf
  pdf: 
    fig-width: 7
    fig-height: 7
---

::: {.callout-important style="font-size: 0.8em;"}
Please read the instructions carefully before submitting your assignment.

1.  This assignment requires you to only upload a `PDF` file on Canvas
2.  Don't collapse any code cells before submitting.
3.  Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::

In this assignment, we will perform various tasks involving principal component analysis (PCA), principal component regression, and dimensionality reduction.

We will need the following packages:

```{R, message=FALSE, warning=FALSE, results='hide'}
packages <- c(
  "tibble",
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "broom",
  "magrittr",
  "corrplot",
  "car"
)
# renv::install(packages)
sapply(packages, require, character.only=T)
```

## <br><br><br><br>

## Question 1

::: callout-tip
## 70 points

Principal component anlaysis and variable selection
:::

###### 1.1 (5 points)

The `data` folder contains a `spending.csv` dataset which is an illustrative sample of monthly spending data for a group of $5000$ people across a variety of categories. The response variable, `income`, is their monthly income, and objective is to predict the `income` for a an individual based on their spending patterns.

Read the data file as a tibble in R. Preprocess the data such that:

1.  the variables are of the right data type, e.g., categorical variables are encoded as factors
2.  all column names to lower case for consistency
3.  Any observations with missing values are dropped

```{R}
path <- "data/spending.csv"

df <- read_csv(path, show_col_types = FALSE) %>%
  rename_with(tolower) %>%
  drop_na() %>%
  mutate(across(where(is.character), as.factor))

```

------------------------------------------------------------------------

###### 1.2 (5 points)

Visualize the correlation between the variables using the `corrplot()` function. What do you observe? What does this mean for the model?

```{R}
df_x <- df %>%
  select(where(is.numeric))

cor_matrix <- cor(df_x, use = "complete.obs")  
corrplot(cor_matrix, method = "circle")

# Due to the overlap of features names, there might be a very large number of variables whicch could imply a very complex model if all these features are to be used.
```

------------------------------------------------------------------------

###### 1.3 (5 points)

Run a linear regression model to predict the `income` variable using the remaining predictors. Interpret the coefficients and summarize your results.

```{R}
model <- lm(income ~ ., data = df_x)
summary(model)

# Notably positive coefficients like laptops, video_games, and electronics suggest that spending on technological items suggest a strong positive correlation with income


```

------------------------------------------------------------------------

###### 1.3 (5 points)

Diagnose the model using the `vif()` function. What do you observe? What does this mean for the model?

```{R}
vif(model)

# Most of the variables hav VIF values in the hundreds and even thousands which indicates strong multicollinearity
# This can lead to problems in the model such that the standard errors of the coefficients are probably inflated, making it harder to detect significant predictors
```

------------------------------------------------------------------------

###### 1.4 (5 points)

Perform PCA using the `princomp` function in R. Print the summary of the PCA object.

```{R}
pca <- princomp(df_x, cor = TRUE, scores = TRUE)
summary(pca)
```

------------------------------------------------------------------------

###### 1.5 (5 points)

Make a screeplot of the proportion of variance explained by each principal component. How many principal components would you choose to keep? Why?

```{R}
var_explained <- pca$sdev^2
prop_var_explained <- var_explained / sum(var_explained)

plot(prop_var_explained, xlab = "Principal Component", ylab = "Proportion of Variance Explained",
     type = "b", pch = 19, main = "Scree Plot")
     
     #  would keep the first two principal components with the reason being that  the first component explains  more variance than the others, and the second component has more of a contribution before it lowers in proportion of variance explained.
```

###### 1.6 (5 points)

By setting any factor loadings below $0.2$ to $0$, summarize the factor loadings for the principal components that you chose to keep.

```{R}
clean_loadings <- loadings(pca)
clean_loadings[abs(clean_loadings) < 0.2] <- 0

factor_loadings_summary <- clean_loadings[, 1:2]
print(factor_loadings_summary)
```

Visualize the factor loadings.

```{R}
heatmap(clean_loadings, 
        Rowv = NULL, Colv = NULL, 
        scale = "none", 
        main = "Factor Loadings Heatmap",
        xlab = "Principal Components", 
        ylab = "Variables")
```

------------------------------------------------------------------------

###### 1.7 (15 points)

Based on the factor loadings, what do you think the principal components represent?

Provide an interpreation for each principal component you chose to keep.

The first principal component probably captures the most variance in the data set. The second principal component captures the variance perpendicular to the first component, representing a different dimension of the data.

------------------------------------------------------------------------

###### 1.8 (10 points)

Create a new data frame with the original response variable `income` and the principal components you chose to keep. Call this data frame `df_pca`.

```{R}
scores <- pca$scores[, 1:2]
income <- df$income
df_pca <- data.frame(income, scores)
names(df_pca)[2:3] <- c("PC1", "PC2")
```

Fit a regression model to predict the `income` variable using the principal components you chose to keep. Interpret the coefficients and summarize your results.

```{R}
lm_pca <- lm(income ~ PC1 + PC2, data = df_pca)
summary(lm_pca)
```

Compare the results of the regression model in 1.3 and 1.9. What do you observe? What does this mean for the model?

```{R}
# In the PCA model, the R-squared value drops  to 0.04696, indicating that the two principal components only explain about 4.7% of the variability in income.The PCA model is more generalizable, however, but needs improvement in predictive accuracy
```

------------------------------------------------------------------------

###### 1.10 (10 points)

Based on your interpretation of the principal components from Question 1.7, provide an interpretation of the regression model in Question 1.9.

The first principal component is positively correlated with income and statistically significant, implying that it captures some underlying pattern in spending that is associated with higher income levels. The second principal component (PC2), however, does not show a statistically significant relationship with income, indicating that it may represent variance in the data that isn't linked to income.

------------------------------------------------------------------------

::: {.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br> <br><br><br><br> ---

::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::
